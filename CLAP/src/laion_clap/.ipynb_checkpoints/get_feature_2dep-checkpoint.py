import numpy as np
import librosa
import torch
import laion_clap
import os
from tqdm import tqdm
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

class EATDFeatureExtractor:
    def __init__(self, model_path, target_sr=48000):
        """
        åˆå§‹åŒ–EATDç‰¹å¾æå–å™¨
        
        Args:
            model_path: CLAPæ¨¡å‹æƒé‡è·¯å¾„
            target_sr: ç›®æ ‡é‡‡æ ·ç‡ï¼ŒCLAPæ¨¡å‹é€šå¸¸ä½¿ç”¨48000Hz
        """
        self.target_sr = target_sr
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆå§‹åŒ–CLAPæ¨¡å‹
        self.model = laion_clap.CLAP_Module(enable_fusion=True)
        self.model.load_ckpt(ckpt=model_path, verbose=True)
        self.model.eval()
        self.model.to(self.device)
        
        print(f"âœ… CLAPæ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")
    
    def load_and_preprocess_audio(self, audio_path, duration=10.0):
        """
        åŠ è½½å¹¶é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            duration: éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡å°†æˆªæ–­ï¼Œä¸è¶³å°†å¡«å……
            
        Returns:
            processed_audio: é¢„å¤„ç†åçš„éŸ³é¢‘æ•°æ®
        """
        try:
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            audio, sr = librosa.load(audio_path, sr=self.target_sr, mono=True)
            
            # è®¡ç®—ç›®æ ‡é•¿åº¦
            target_length = int(duration * self.target_sr)
            current_length = len(audio)
            
            if current_length > target_length:
                # æˆªæ–­éŸ³é¢‘
                audio = audio[:target_length]
            elif current_length < target_length:
                # å¡«å……éŸ³é¢‘
                padding = target_length - current_length
                audio = np.pad(audio, (0, padding), mode='constant')
            
            # ç¡®ä¿éŸ³é¢‘åœ¨[-1, 1]èŒƒå›´å†…
            audio = np.clip(audio, -1.0, 1.0)
            
            return audio.astype('float32')
            
        except Exception as e:
            print(f"âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: {audio_path}, é”™è¯¯: {e}")
            return None
    
    def extract_features(self, audio_data):
        """
        æå–éŸ³é¢‘ç‰¹å¾
        
        Args:
            audio_data: é¢„å¤„ç†åçš„éŸ³é¢‘æ•°æ®
            
        Returns:
            features: éŸ³é¢‘ç‰¹å¾å‘é‡
        """
        if audio_data is None:
            return None
            
        try:
            # å°†éŸ³é¢‘æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼
            # CLAPæ¨¡å‹æœŸæœ›å½¢çŠ¶ä¸º (batch_size, audio_length)
            audio_tensor = torch.from_numpy(audio_data).unsqueeze(0).to(self.device)
            
            # æå–ç‰¹å¾
            with torch.no_grad():
                features = self.model.get_audio_embedding_from_data(
                    audio_tensor, 
                    use_tensor=True
                )
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶è¿”å›
            return features.cpu().numpy().squeeze()
            
        except Exception as e:
            print(f"âŒ ç‰¹å¾æå–å¤±è´¥: {e}")
            return None
    
    def process_dataset(self, dataset_path, output_path, emotion_types=None):
        """
        æ‰¹é‡å¤„ç†æ•´ä¸ªæ•°æ®é›†
        
        Args:
            dataset_path: é‡æ•´åæ•°æ®é›†çš„è·¯å¾„
            output_path: ç‰¹å¾ä¿å­˜è·¯å¾„
            emotion_types: è¦å¤„ç†çš„æƒ…æ„Ÿç±»å‹åˆ—è¡¨
        """
        if emotion_types is None:
            emotion_types = ['negative_out', 'neutral_out', 'positive_out']
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_path, exist_ok=True)
        
        # å­˜å‚¨æ‰€æœ‰ç‰¹å¾å’Œæ ‡ç­¾
        all_features = []
        all_labels = []
        all_file_paths = []
        all_emotion_types = []
        all_splits = []  # è®­ç»ƒé›†æˆ–æµ‹è¯•é›†æ ‡è¯†
        
        # éå†æ•°æ®é›†ç›®å½•ç»“æ„
        splits = ['train', 'val']  # å¯¹åº”é‡æ•´åçš„trainå’Œvalç›®å½•
        
        for split in splits:
            split_path = os.path.join(dataset_path, split)
            if not os.path.exists(split_path):
                print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {split_path}")
                continue
                
            for category in ['depressed', 'non_depressed']:
                category_path = os.path.join(split_path, category)
                if not os.path.exists(category_path):
                    print(f"âš ï¸ è·³è¿‡ä¸å­˜åœ¨çš„ç›®å½•: {category_path}")
                    continue
                
                print(f"\nğŸ” å¤„ç†: {split}/{category}")
                
                # è·å–è¯¥ç±»åˆ«ä¸‹çš„æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
                audio_files = []
                for emotion in emotion_types:
                    pattern = f"*{emotion}.wav"
                    emotion_files = [f for f in os.listdir(category_path) if f.endswith(f'{emotion}.wav')]
                    audio_files.extend([(f, emotion) for f in emotion_files])
                
                if not audio_files:
                    print(f"âš ï¸ åœ¨ {category_path} ä¸­æœªæ‰¾åˆ°éŸ³é¢‘æ–‡ä»¶")
                    continue
                
                # å¤„ç†æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶
                for audio_file, emotion in tqdm(audio_files, desc=f"å¤„ç†{category}"):
                    audio_path = os.path.join(category_path, audio_file)
                    
                    # åŠ è½½å’Œé¢„å¤„ç†éŸ³é¢‘
                    audio_data = self.load_and_preprocess_audio(audio_path)
                    if audio_data is None:
                        continue
                    
                    # æå–ç‰¹å¾
                    featureforSave = []
                    features = self.extract_features(audio_data)
                    featureforSave = np.array(features)

                    
                    if features is not None:
                        all_features.append(features)
                        all_file_paths.append(audio_path)
                        all_emotion_types.append(emotion)
                        all_splits.append(split)
                        
                        # æ ¹æ®ç›®å½•ç»“æ„ç¡®å®šæ ‡ç­¾
                        label = 1 if category == 'depressed' else 0
                        all_labels.append(label)

                        # feature_filename = os.path.splitext(audio_file)[0] + '.npy'
                        
                        feature_file_path = os.path.join(output_path, split, category)
                                
                        # ä¿å­˜ç‰¹å¾æ–‡ä»¶
                        np.save(feature_file_path, features)
                    
                
                    
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        all_features = np.array(all_features)
        all_labels = np.array(all_labels)
        
        # ä¿å­˜ç‰¹å¾å’Œå…ƒæ•°æ®
        self.save_features(all_features, all_labels, all_file_paths, 
                          all_emotion_types, all_splits, output_path)
        
        return all_features, all_labels
    
    def save_features(self, features, labels, file_paths, emotion_types, splits, output_path):
        """
        ä¿å­˜ç‰¹å¾å’Œå…ƒæ•°æ®
        """
        # ä¿å­˜ç‰¹å¾æ•°ç»„
        np.save(os.path.join(output_path, 'audio_features.npy'), features)
        np.save(os.path.join(output_path, 'audio_labels.npy'), labels)
        
        # ä¿å­˜å…ƒæ•°æ®ä¸ºCSV
        metadata = {
            'file_path': file_paths,
            'label': labels,
            'emotion_type': emotion_types,
            'split': splits
        }
        
        df_metadata = pd.DataFrame(metadata)
        df_metadata.to_csv(os.path.join(output_path, 'metadata.csv'), index=False)
        
        # ä¿å­˜ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        feature_stats = {
            'total_samples': len(features),
            'feature_dim': features.shape[1] if len(features.shape) > 1 else features.shape[0],
            'depressed_count': np.sum(labels == 1),
            'non_depressed_count': np.sum(labels == 0),
            'train_count': np.sum(np.array(splits) == 'train'),
            'val_count': np.sum(np.array(splits) == 'val')
        }
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š ç‰¹å¾æå–å®Œæˆï¼")
        print(f"ğŸ“ æ€»æ ·æœ¬æ•°: {feature_stats['total_samples']}")
        print(f"ğŸ”¢ ç‰¹å¾ç»´åº¦: {feature_stats['feature_dim']}")
        print(f"ğŸ˜” æŠ‘éƒæ ·æœ¬: {feature_stats['depressed_count']}")
        print(f"ğŸ˜Š éæŠ‘éƒæ ·æœ¬: {feature_stats['non_depressed_count']}")
        print(f"ğŸ‹ï¸ è®­ç»ƒé›†æ ·æœ¬: {feature_stats['train_count']}")
        print(f"ğŸ§ª éªŒè¯é›†æ ·æœ¬: {feature_stats['val_count']}")
        print(f"ğŸ’¾ ç‰¹å¾ä¿å­˜è·¯å¾„: {output_path}")
    
    def load_saved_features(self, feature_path):
        """
        åŠ è½½å·²ä¿å­˜çš„ç‰¹å¾
        """
        features = np.load(os.path.join(feature_path, 'audio_features.npy'))
        labels = np.load(os.path.join(feature_path, 'audio_labels.npy'))
        metadata = pd.read_csv(os.path.join(feature_path, 'metadata.csv'))
        
        return features, labels, metadata

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    MODEL_PATH = "/root/model/630k-audioset-fusion-best.pt"
    DATASET_PATH = "/root/dataset/EATD-2classification"  # æ›¿æ¢ä¸ºæ‚¨çš„é‡æ•´åæ•°æ®é›†è·¯å¾„
    OUTPUT_PATH = "/root/CLAP-Embedding/features"      # æ›¿æ¢ä¸ºæ‚¨å¸Œæœ›ä¿å­˜ç‰¹å¾çš„è·¯å¾„
    
    # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
    extractor = EATDFeatureExtractor(model_path=MODEL_PATH)
    
    # å¤„ç†æ•´ä¸ªæ•°æ®é›†
    features, labels = extractor.process_dataset(
        dataset_path=DATASET_PATH,
        output_path=OUTPUT_PATH
    )
    
    print("âœ… ç‰¹å¾æå–æµç¨‹å®Œæˆï¼")