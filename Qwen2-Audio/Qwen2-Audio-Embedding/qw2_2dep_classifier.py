import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')


class NPYDepressionDataset(Dataset):
    """ç›´æ¥ä»npyæ–‡ä»¶åŠ è½½çš„æŠ‘éƒç—‡åˆ†ç±»æ•°æ®é›†ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, features_base_path, split='train'):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        Args:
            features_base_path: ç‰¹å¾æ–‡ä»¶æ ¹ç›®å½•
            split: æ•°æ®é›†åˆ†å‰² ('train' æˆ– 'val')
        """
        self.split = split
        self.split_path = os.path.join(features_base_path, split)

        if not os.path.exists(self.split_path):
            raise ValueError(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {self.split_path}")

        # åŠ è½½æŠ‘éƒå’ŒéæŠ‘éƒç‰¹å¾æ–‡ä»¶
        depressed_path = os.path.join(self.split_path, 'depressed_embeddings.npy')
        non_depressed_path = os.path.join(self.split_path, 'non_depressed_embeddings.npy')

        if not os.path.exists(depressed_path):
            raise ValueError(f"âŒ æŠ‘éƒç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {depressed_path}")
        if not os.path.exists(non_depressed_path):
            raise ValueError(f"âŒ éæŠ‘éƒç‰¹å¾æ–‡ä»¶ä¸å­˜åœ¨: {non_depressed_path}")

        # åŠ è½½ç‰¹å¾æ•°æ®
        depressed_features = np.load(depressed_path)
        non_depressed_features = np.load(non_depressed_path)

        print(f"ğŸ” åŸå§‹ç‰¹å¾å½¢çŠ¶ - æŠ‘éƒ: {depressed_features.shape}, éæŠ‘éƒ: {non_depressed_features.shape}")

        # ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾æ˜¯äºŒç»´çš„ (n_samples, feature_dim)
        if len(depressed_features.shape) == 1:
            depressed_features = depressed_features.reshape(-1, 1)
        if len(non_depressed_features.shape) == 1:
            non_depressed_features = non_depressed_features.reshape(-1, 1)

        # åˆå¹¶ç‰¹å¾å’Œåˆ›å»ºæ ‡ç­¾
        self.features = np.concatenate([depressed_features, non_depressed_features], axis=0)
        self.labels = np.concatenate([
            np.ones(len(depressed_features)),  # æŠ‘éƒæ ‡ç­¾ä¸º1
            np.zeros(len(non_depressed_features))  # éæŠ‘éƒæ ‡ç­¾ä¸º0
        ], axis=0)

        print(f"âœ… æˆåŠŸåŠ è½½ {split} æ•°æ®é›†:")
        print(f"   - æŠ‘éƒæ ·æœ¬æ•°: {len(depressed_features)}")
        print(f"   - éæŠ‘éƒæ ·æœ¬æ•°: {len(non_depressed_features)}")
        print(f"   - æ€»æ ·æœ¬æ•°: {len(self.features)}")
        print(f"   - ç‰¹å¾ç»´åº¦: {self.features.shape[1]}")
        print(f"   - ç‰¹å¾æ•°ç»„å½¢çŠ¶: {self.features.shape}")

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = self.features[idx]
        label = self.labels[idx]

        # ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾æ˜¯1Dæ•°ç»„ï¼ˆå³ä½¿ç»´åº¦ä¸º1ï¼‰
        if len(feature.shape) > 0:
            feature = feature.flatten()

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        feature_tensor = torch.FloatTensor(feature)
        label_tensor = torch.LongTensor([int(label)])  # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°

        return feature_tensor, label_tensor.squeeze()


class DepressionClassifier(nn.Module):
    """æŠ‘éƒç—‡äºŒåˆ†ç±»æ¨¡å‹ï¼ˆé€‚é…ä½ç»´ç‰¹å¾ï¼‰"""

    def __init__(self, input_dim, hidden_dims=[128, 64], dropout_rate=0.3):
        """
        åˆå§‹åŒ–åˆ†ç±»å™¨ï¼ˆé€‚é…ä½ç»´ç‰¹å¾ï¼‰

        Args:
            input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dims: éšè—å±‚ç»´åº¦åˆ—è¡¨ï¼ˆæ ¹æ®è¾“å…¥ç»´åº¦è°ƒæ•´ï¼‰
            dropout_rate: dropoutæ¯”ç‡
        """
        super(DepressionClassifier, self).__init__()

        # æ ¹æ®è¾“å…¥ç»´åº¦åŠ¨æ€è°ƒæ•´ç½‘ç»œç»“æ„
        if input_dim <= 10:  # ä½ç»´ç‰¹å¾
            hidden_dims = [max(32, input_dim * 4), max(16, input_dim * 2)]
        elif input_dim <= 100:  # ä¸­ç­‰ç»´åº¦
            hidden_dims = [256, 128]
        else:  # é«˜ç»´ç‰¹å¾
            hidden_dims = [512, 256, 128]

        layers = []
        prev_dim = input_dim

        # åŠ¨æ€æ„å»ºéšè—å±‚
        for i, hidden_dim in enumerate(hidden_dims):
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim) if hidden_dim > 1 else nn.Identity(),  # ä½ç»´æ—¶è·³è¿‡BatchNorm
                nn.ReLU(),
                nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()
            ])
            prev_dim = hidden_dim

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, 2))  # äºŒåˆ†ç±»è¾“å‡º

        self.classifier = nn.Sequential(*layers)

    def forward(self, x):
        # ä¿®å¤ï¼šç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if len(x.shape) == 1:
            x = x.unsqueeze(0)  # å¦‚æœæ˜¯1Dï¼Œæ·»åŠ batchç»´åº¦
        return self.classifier(x)


class DepressionTrainer:
    """æŠ‘éƒç—‡åˆ†ç±»è®­ç»ƒå™¨ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, model, train_loader, val_loader, device='cuda', learning_rate=1e-4):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=5, factor=0.5
        )

        # è®­ç»ƒå†å²è®°å½•
        self.train_losses = []
        self.val_losses = []
        self.train_accuracies = []
        self.val_accuracies = []

    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.model.train()
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        pbar = tqdm(self.train_loader, desc=f"è®­ç»ƒä¸­")
        for batch_idx, (features, labels) in enumerate(pbar):
            try:
                # ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾å½¢çŠ¶æ­£ç¡®
                if len(features.shape) == 1:
                    features = features.unsqueeze(1)  # å¦‚æœæ˜¯1Dï¼Œæ·»åŠ ç‰¹å¾ç»´åº¦

                features = features.to(self.device)
                labels = labels.to(self.device)

                # å‰å‘ä¼ æ’­
                self.optimizer.zero_grad()
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)

                # åå‘ä¼ æ’­
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                # ç»Ÿè®¡ä¿¡æ¯
                running_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                total_samples += labels.size(0)
                correct_predictions += (predicted == labels).sum().item()

                # æ›´æ–°è¿›åº¦æ¡
                current_acc = 100.0 * correct_predictions / total_samples
                current_loss = running_loss / (batch_idx + 1)
                pbar.set_postfix({
                    'Loss': f'{current_loss:.4f}',
                    'Acc': f'{current_acc:.2f}%'
                })

            except Exception as e:
                print(f"âŒ æ‰¹å¤„ç† {batch_idx} å‡ºé”™: {e}")
                print(f"   ç‰¹å¾å½¢çŠ¶: {features.shape}")
                print(f"   æ ‡ç­¾å½¢çŠ¶: {labels.shape}")
                continue

        epoch_loss = running_loss / len(self.train_loader)
        epoch_accuracy = 100.0 * correct_predictions / total_samples

        self.train_losses.append(epoch_loss)
        self.train_accuracies.append(epoch_accuracy)

        return epoch_loss, epoch_accuracy

    def validate_epoch(self):
        """éªŒè¯ä¸€ä¸ªepochï¼ˆä¿®å¤ç‰ˆï¼‰"""
        self.model.eval()
        running_loss = 0.0
        correct_predictions = 0
        total_samples = 0

        all_predictions = []
        all_labels = []

        with torch.no_grad():
            pbar = tqdm(self.val_loader, desc=f"éªŒè¯ä¸­")
            for features, labels in pbar:
                try:
                    # ä¿®å¤ï¼šç¡®ä¿ç‰¹å¾å½¢çŠ¶æ­£ç¡®
                    if len(features.shape) == 1:
                        features = features.unsqueeze(1)

                    features = features.to(self.device)
                    labels = labels.to(self.device)

                    outputs = self.model(features)
                    loss = self.criterion(outputs, labels)

                    running_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    total_samples += labels.size(0)
                    correct_predictions += (predicted == labels).sum().item()

                    all_predictions.extend(predicted.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())

                    # æ›´æ–°è¿›åº¦æ¡
                    current_acc = 100.0 * correct_predictions / total_samples
                    pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})

                except Exception as e:
                    print(f"âŒ éªŒè¯æ‰¹å¤„ç†å‡ºé”™: {e}")
                    continue

        epoch_loss = running_loss / len(self.val_loader) if len(self.val_loader) > 0 else 0
        epoch_accuracy = 100.0 * correct_predictions / total_samples if total_samples > 0 else 0

        # è®¡ç®—å…¶ä»–æŒ‡æ ‡
        precision = precision_score(all_labels, all_predictions, average='binary', zero_division=0) if len(
            all_labels) > 0 else 0
        recall = recall_score(all_labels, all_predictions, average='binary', zero_division=0) if len(
            all_labels) > 0 else 0
        f1 = f1_score(all_labels, all_predictions, average='binary', zero_division=0) if len(all_labels) > 0 else 0

        self.val_losses.append(epoch_loss)
        self.val_accuracies.append(epoch_accuracy)

        return epoch_loss, epoch_accuracy, precision, recall, f1, all_predictions, all_labels

    def train(self, epochs=50, save_path='best_depression_classifier.pth'):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("ğŸš€ å¼€å§‹è®­ç»ƒæŠ‘éƒç—‡åˆ†ç±»æ¨¡å‹...")
        best_accuracy = 0.0

        for epoch in range(epochs):
            print(f"\nğŸ“Š Epoch {epoch + 1}/{epochs}")
            print("-" * 50)

            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch()

            # éªŒè¯
            val_loss, val_acc, precision, recall, f1, _, _ = self.validate_epoch()

            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step(val_loss)

            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"éªŒè¯æŸå¤±: {val_loss:.4f}, éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            print(f"ç²¾ç¡®ç‡: {precision:.4f}, å¬å›ç‡: {recall:.4f}, F1åˆ†æ•°: {f1:.4f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_accuracy:
                best_accuracy = val_acc
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'best_accuracy': best_accuracy,
                    'input_dim': self.model.classifier[0].in_features
                }, save_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {best_accuracy:.2f}%")

        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_accuracy:.2f}%")


def debug_dataset_shapes(features_base_path):
    """è°ƒè¯•å‡½æ•°ï¼šæ£€æŸ¥æ•°æ®é›†å½¢çŠ¶"""
    print("ğŸ› è°ƒè¯•ä¿¡æ¯:")

    for split in ['train', 'val']:
        split_path = os.path.join(features_base_path, split)
        if os.path.exists(split_path):
            depressed_path = os.path.join(split_path, 'depressed_embeddings.npy')
            non_depressed_path = os.path.join(split_path, 'non_depressed_embeddings.npy')

            if os.path.exists(depressed_path):
                depressed_features = np.load(depressed_path)
                print(f"{split}/depressed.npy - å½¢çŠ¶: {depressed_features.shape}, æ•°æ®ç±»å‹: {depressed_features.dtype}")

            if os.path.exists(non_depressed_path):
                non_depressed_features = np.load(non_depressed_path)
                print(
                    f"{split}/non_depressed.npy - å½¢çŠ¶: {non_depressed_features.shape}, æ•°æ®ç±»å‹: {non_depressed_features.dtype}")


def main():
    """ä¸»å‡½æ•°ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    # é…ç½®è·¯å¾„
    FEATURES_BASE_PATH = "./2-Dep-Classification/"  # ç‰¹å¾æ–‡ä»¶æ ¹ç›®å½•
    MODEL_SAVE_PATH = "best_depression_classifier.pth"

    # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®é›†å½¢çŠ¶
    debug_dataset_shapes(FEATURES_BASE_PATH)

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(FEATURES_BASE_PATH):
        print(f"âŒ ç‰¹å¾è·¯å¾„ä¸å­˜åœ¨: {FEATURES_BASE_PATH}")
        return

    try:
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = NPYDepressionDataset(FEATURES_BASE_PATH, split='train')
        val_dataset = NPYDepressionDataset(FEATURES_BASE_PATH, split='val')

        # è·å–ç‰¹å¾ç»´åº¦
        feature_dim = train_dataset.features.shape[1]
        print(f"ğŸ”¢ æœ€ç»ˆä½¿ç”¨çš„ç‰¹å¾ç»´åº¦: {feature_dim}")

        # è®¾ç½®è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¿®å¤ï¼šè®¾ç½®drop_last=Trueé¿å…æœ€åä¸€ä¸ªbatché—®é¢˜ï¼‰
        batch_size = 32
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True)

        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"ğŸ“Š éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        print(f"ğŸ“¦ æ‰¹å¤§å°: {batch_size}")

        # åˆ›å»ºæ¨¡å‹
        model = DepressionClassifier(input_dim=feature_dim)
        print(f"ğŸ§  æ¨¡å‹ç»“æ„:")
        print(model)
        print(f"ğŸ“ æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

        # è°ƒæ•´å­¦ä¹ ç‡ï¼ˆæ ¹æ®ç‰¹å¾ç»´åº¦ï¼‰
        learning_rate = 1e-3 if feature_dim <= 10 else 1e-4
        print(f"ğŸ“š ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")

        # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        trainer = DepressionTrainer(model, train_loader, val_loader, device=device, learning_rate=learning_rate)
        trainer.train(epochs=30, save_path=MODEL_SAVE_PATH)  # å‡å°‘epochæ•°ç”¨äºæµ‹è¯•

    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return


if __name__ == "__main__":
    main()